<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content="Timo "><meta name=description content="Introduction Large Language Models (LLMs) excel in generating text but often struggle to produce structured output. By leveraging Pydantic&amp;rsquo;s type validation and prompt engineering, we can enforce and validate the output generated by LLMs.
All code examples in this blog post are written in Python. The LLM used is OpenAI&amp;rsquo;s gpt-3.5-turbo.
Query the LLM To query the LLM, we use the following function:
import openai def query(prompt: str) -&amp;gt; str: &amp;#34;&amp;#34;&amp;#34;Query the LLM with the given prompt."><meta name=keywords content=",python,llm,openai,gpt,pydantic"><meta name=robots content="noodp"><meta name=theme-color content><link rel=canonical href=https://timotk.github.io/posts/2023/08/enforce-and-validate-llm-output-with-pydantic/><title>Enforce and Validate LLM Output with Pydantic :: Timo â€” My simple blog</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=https://timotk.github.io/main.4e5c639214707eff609bb55fe49e183dee42258a73bc90e4cc7b0a84f900798a.css><link rel=apple-touch-icon sizes=180x180 href=https://timotk.github.io/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://timotk.github.io/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://timotk.github.io/favicon-16x16.png><link rel=manifest href=https://timotk.github.io/site.webmanifest><link rel=mask-icon href=https://timotk.github.io/safari-pinned-tab.svg color><link rel="shortcut icon" href=https://timotk.github.io/favicon.ico><meta name=msapplication-TileColor content><meta itemprop=name content="Enforce and Validate LLM Output with Pydantic"><meta itemprop=description content="Introduction Large Language Models (LLMs) excel in generating text but often struggle to produce structured output. By leveraging Pydantic&rsquo;s type validation and prompt engineering, we can enforce and validate the output generated by LLMs.
All code examples in this blog post are written in Python. The LLM used is OpenAI&rsquo;s gpt-3.5-turbo.
Query the LLM To query the LLM, we use the following function:
import openai def query(prompt: str) -> str: &#34;&#34;&#34;Query the LLM with the given prompt."><meta itemprop=datePublished content="2023-08-07T13:00:00+02:00"><meta itemprop=dateModified content="2023-08-07T13:00:00+02:00"><meta itemprop=wordCount content="892"><meta itemprop=image content="https://timotk.github.io"><meta itemprop=keywords content="python,llm,openai,gpt,pydantic,"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://timotk.github.io"><meta name=twitter:title content="Enforce and Validate LLM Output with Pydantic"><meta name=twitter:description content="Introduction Large Language Models (LLMs) excel in generating text but often struggle to produce structured output. By leveraging Pydantic&rsquo;s type validation and prompt engineering, we can enforce and validate the output generated by LLMs.
All code examples in this blog post are written in Python. The LLM used is OpenAI&rsquo;s gpt-3.5-turbo.
Query the LLM To query the LLM, we use the following function:
import openai def query(prompt: str) -> str: &#34;&#34;&#34;Query the LLM with the given prompt."><meta property="og:title" content="Enforce and Validate LLM Output with Pydantic"><meta property="og:description" content="Introduction Large Language Models (LLMs) excel in generating text but often struggle to produce structured output. By leveraging Pydantic&rsquo;s type validation and prompt engineering, we can enforce and validate the output generated by LLMs.
All code examples in this blog post are written in Python. The LLM used is OpenAI&rsquo;s gpt-3.5-turbo.
Query the LLM To query the LLM, we use the following function:
import openai def query(prompt: str) -> str: &#34;&#34;&#34;Query the LLM with the given prompt."><meta property="og:type" content="article"><meta property="og:url" content="https://timotk.github.io/posts/2023/08/enforce-and-validate-llm-output-with-pydantic/"><meta property="og:image" content="https://timotk.github.io"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-07T13:00:00+02:00"><meta property="article:modified_time" content="2023-08-07T13:00:00+02:00"><meta property="article:published_time" content="2023-08-07 13:00:00 +0200 +0200"></head><body><div class=container><header class=header><span class=header__inner><a href=https://timotk.github.io/ style=text-decoration:none><div class=logo><span class=logo__mark>></span>
<span class=logo__text>Timo</span>
<span class=logo__cursor style=background-color:#67a2c9;animation-duration:2s></span></div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=https://timotk.github.io/about/>About</a></li><li><a href=https://timotk.github.io/posts/>Posts</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span><span class="theme-toggle not-selectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41c10.4934.0 19-8.5066 19-19C41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><main class=post><div class=post-info><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>5 minutes</p></div><article><h1 class=post-title><a href=https://timotk.github.io/posts/2023/08/enforce-and-validate-llm-output-with-pydantic/>Enforce and Validate LLM Output with Pydantic</a></h1><div class=post-content><h1 id=introduction>Introduction</h1><p>Large Language Models (LLMs) excel in generating text but often struggle to produce structured output.
By leveraging <a href=https://docs.pydantic.dev/latest/>Pydantic</a>&rsquo;s type validation and prompt engineering, we can enforce and validate the output generated by LLMs.</p><p><em>All code examples in this blog post are written in Python. The LLM used is <a href=https://platform.openai.com/docs/guides/gpt>OpenAI&rsquo;s gpt-3.5-turbo</a>.</em></p><h1 id=query-the-llm>Query the LLM</h1><p>To query the LLM, we use the following function:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> openai
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>query</span>(prompt: str) <span style=color:#f92672>-&gt;</span> str:
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Query the LLM with the given prompt.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    completion <span style=color:#f92672>=</span> openai<span style=color:#f92672>.</span>ChatCompletion<span style=color:#f92672>.</span>create(
</span></span><span style=display:flex><span>        model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;gpt-3.5-turbo&#34;</span>,
</span></span><span style=display:flex><span>        messages<span style=color:#f92672>=</span>[
</span></span><span style=display:flex><span>            {
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;content&#34;</span>: prompt,
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>        ],
</span></span><span style=display:flex><span>        temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> completion<span style=color:#f92672>.</span>choices[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>message<span style=color:#f92672>.</span>content
</span></span></code></pre></div><h1 id=query-the-model>Query the model</h1><p>We can query the model with a simple question:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>response <span style=color:#f92672>=</span> query(<span style=color:#e6db74>&#34;What is the largest planet in our solar system?&#34;</span>)
</span></span><span style=display:flex><span>print(response)
</span></span><span style=display:flex><span><span style=color:#e6db74>&#39;The largest planet in our solar system is Jupiter.&#39;</span>
</span></span></code></pre></div><h1 id=enforcing-json-output-with-a-prompt>Enforcing JSON output with a prompt</h1><p>In our prompt, we can ask the LLM to respond in a certain format:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>I will ask you questions and you will respond. Your response should be in the following format:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>```json
</span></span></span><span style=display:flex><span><span style=color:#e6db74>{
</span></span></span><span style=display:flex><span><span style=color:#e6db74>	&#34;thought&#34;: &#34;How you think about the question&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>	&#34;answer&#34;: &#34;The answer to the question&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>}
</span></span></span><span style=display:flex><span><span style=color:#e6db74>```
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;</span>
</span></span></code></pre></div><p>Then, we query the model:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>question <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;What is the largest planet in our solar system?&#34;</span>
</span></span><span style=display:flex><span>response <span style=color:#f92672>=</span> query(prompt <span style=color:#f92672>+</span> question)
</span></span><span style=display:flex><span>print(response)
</span></span><span style=display:flex><span><span style=color:#e6db74>&#39;{</span>
</span></span><span style=display:flex><span>	<span style=color:#e6db74>&#34;thought&#34;</span>: <span style=color:#e6db74>&#34;This is a factual question that can be answered with scientific knowledge.&#34;</span>,
</span></span><span style=display:flex><span>	<span style=color:#e6db74>&#34;answer&#34;</span>: <span style=color:#e6db74>&#34;The largest planet in our solar system is Jupiter.&#34;</span>
</span></span><span style=display:flex><span>}<span style=color:#e6db74>&#39;</span>
</span></span></code></pre></div><p>This is great, because we can easily parse the structured output:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> json
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>parsed_response <span style=color:#f92672>=</span> json<span style=color:#f92672>.</span>loads(response)
</span></span><span style=display:flex><span>print(parsed_response[<span style=color:#e6db74>&#34;answer&#34;</span>])
</span></span><span style=display:flex><span><span style=color:#e6db74>&#39;The largest planet in our solar system is Jupiter.&#39;</span>
</span></span></code></pre></div><h1 id=validating-the-output>Validating the output</h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pydantic <span style=color:#f92672>import</span> BaseModel
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ThoughtAnswerResponse</span>(BaseModel):
</span></span><span style=display:flex><span>    thought: str
</span></span><span style=display:flex><span>    answer: str
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>raw_response <span style=color:#f92672>=</span> query(prompt)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Note: When you are using pydantic&lt;2.0, use parse_raw instead of model_validate_json</span>
</span></span><span style=display:flex><span>validated_response <span style=color:#f92672>=</span> ThoughtAnswerResponse<span style=color:#f92672>.</span>model_validate_json(raw_response)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(validated_response)
</span></span><span style=display:flex><span>thought<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;This is a factual question that can be answered with scientific knowledge.&#39;</span> answer<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;The largest planet in our solar system is Jupiter.&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(type(validated_response))
</span></span><span style=display:flex><span><span style=color:#f92672>&lt;</span><span style=color:#66d9ef>class</span> <span style=color:#960050;background-color:#1e0010>&#39;</span><span style=color:#a6e22e>ThoughtAnswerResponse</span><span style=color:#e6db74>&#39;&gt;</span>
</span></span></code></pre></div><h1 id=using-the-pydantic-model-in-the-prompt>Using the Pydantic model in the prompt</h1><p>At this moment, we describe our response format in two places:</p><ul><li>a JSON description in our prompt</li><li>a corresponding Pydantic model</li></ul><p>When we want to update the response format, we need to change both the prompt and the Pydantic model. This can cause inconsistencies.</p><p>We can solve this by <a href=https://docs.pydantic.dev/latest/usage/json_schema/>exporting the Pydantic model to a JSON schema</a> and adding the schema to the prompt. This will make the response and the Pydantic model consistent.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>response_schema_dict <span style=color:#f92672>=</span> ThoughtAnswerResponse<span style=color:#f92672>.</span>model_json_schema()
</span></span><span style=display:flex><span>response_schema_json <span style=color:#f92672>=</span> json<span style=color:#f92672>.</span>dumps(response_schema_dict, indent<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>I will ask you questions, and you will respond.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>Your response should be in the following format:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>```json
</span></span></span><span style=display:flex><span><span style=color:#e6db74></span><span style=color:#e6db74>{</span>response_schema_json<span style=color:#e6db74>}</span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>```
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;</span>
</span></span></code></pre></div><p>The prompt will now look like this:</p><pre tabindex=0><code>I will ask you questions, and you will respond. Your response should be in the following format:
```json
{
	&#34;properties&#34;: {
		&#34;thought&#34;: { &#34;title&#34;: &#34;Thought&#34;, &#34;type&#34;: &#34;string&#34; },
		&#34;answer&#34;: { &#34;title&#34;: &#34;Answer&#34;, &#34;type&#34;: &#34;string&#34; }
	},
	&#34;required&#34;: [&#34;thought&#34;, &#34;answer&#34;],
	&#34;title&#34;: &#34;ThoughtAnswerResponse&#34;,
	&#34;type&#34;: &#34;object&#34;
}
</code></pre><p>The response will look like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;thought&#34;</span>: <span style=color:#e6db74>&#34;The largest planet in our solar system is Jupiter.&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;answer&#34;</span>: <span style=color:#e6db74>&#34;Jupiter&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Now, whenever you change the Pydantic model, the corresponding schema will be put in the prompt. Note that the schema has become more complex than it was before. One benefit is that it allows us to be more specific in what responses we require.</p><h1 id=error-handling>Error handling</h1><p>The LLM may still produce results that are not consistent with our model. We can add some code to catch this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pydantic <span style=color:#f92672>import</span> ValidationError
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>    validated_response <span style=color:#f92672>=</span> ThoughtAnswerResponse<span style=color:#f92672>.</span>model_validate_json(raw_response)
</span></span><span style=display:flex><span><span style=color:#66d9ef>except</span> ValidationError <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Unable to validate LLM response.&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># Add your own error handling here</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>raise</span> e
</span></span></code></pre></div><h1 id=enforce-specific-values-using-a-literal>Enforce specific values using a Literal</h1><p>Sometimes, you want to enforce the use of specific values for a given field. We add the field &ldquo;difficulty&rdquo; to our response object. The LLM should use it to provide information about the difficulty of the question. In a regular prompt, we would do the following:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;&#34;Your response should be in the following format:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>```json
</span></span></span><span style=display:flex><span><span style=color:#e6db74>{
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  &#34;thought&#34;: &#34;How you think about the question&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  &#34;answer&#34;: &#34;The answer to the question&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  &#34;difficulty&#34;: &#34;How difficult the question was. One of easy, medium or hard&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>}
</span></span></span><span style=display:flex><span><span style=color:#e6db74>```
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;</span>
</span></span></code></pre></div><p>Of course, the model could potentially still use other values. To validate it, we would need to write custom code.</p><p>With Pydantic, it is a lot easier. We create a new type called <code>Difficulty</code> using a <a href=https://docs.python.org/3/library/typing.html#typing.Literal>Literal</a>.
A Literal allows us to specify the use of a select list of values.
We add a <code>Difficulty</code> type hint to the <code>difficulty</code> field in our Pydantic model:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> typing <span style=color:#f92672>import</span> Literal
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pydantic <span style=color:#f92672>import</span> BaseModel
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># We create a new type</span>
</span></span><span style=display:flex><span>Difficulty <span style=color:#f92672>=</span> Literal[<span style=color:#e6db74>&#34;easy&#34;</span>, <span style=color:#e6db74>&#34;medium&#34;</span>, <span style=color:#e6db74>&#34;hard&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ThoughtAnswerResponse</span>(BaseModel):
</span></span><span style=display:flex><span>    thought: str
</span></span><span style=display:flex><span>    answer: str
</span></span><span style=display:flex><span>    difficulty: Difficulty
</span></span></code></pre></div><p>The LLM responds may respond with a value we do not allow:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;thought&#34;</span>: <span style=color:#e6db74>&#34;The largest planet in our solar system is Jupiter.&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;answer&#34;</span>: <span style=color:#e6db74>&#34;Jupiter&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;difficulty&#34;</span>: <span style=color:#e6db74>&#34;Unknown&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>When we parse this result, Pydantic will validate the values for the <code>difficulty</code> field. <code>Unknown</code> does not match one of the values specified in the Literal type we have defined. So we get the following error:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>validated_response <span style=color:#f92672>=</span> ThoughtAnswerResponse<span style=color:#f92672>.</span>model_validate_json(response)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ValidationError: <span style=color:#ae81ff>1</span> validation error <span style=color:#66d9ef>for</span> ThoughtAnswerResponse
</span></span><span style=display:flex><span>difficulty
</span></span><span style=display:flex><span>    Input should be <span style=color:#e6db74>&#39;easy&#39;</span>, <span style=color:#e6db74>&#39;medium&#39;</span> <span style=color:#f92672>or</span> <span style=color:#e6db74>&#39;hard&#39;</span> [type<span style=color:#f92672>=</span>literal_error, input_value<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Unknown&#39;</span>, input_type<span style=color:#f92672>=</span>str]
</span></span></code></pre></div><h1 id=conclusion>Conclusion</h1><p>By using Pydantic and prompt engineering, you can enforce and validate the output of LLMs. This provides you with greater control of the LLM output and allow you to build more robust AI systems.</p></div></article><hr><div class=post-info><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg><span class=tag><a href=https://timotk.github.io/tags/python/>python</a></span>
<span class=tag><a href=https://timotk.github.io/tags/llm/>llm</a></span>
<span class=tag><a href=https://timotk.github.io/tags/openai/>openai</a></span>
<span class=tag><a href=https://timotk.github.io/tags/gpt/>gpt</a></span>
<span class=tag><a href=https://timotk.github.io/tags/pydantic/>pydantic</a></span></p><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="16" y1="13" x2="8" y2="13"/><line x1="16" y1="17" x2="8" y2="17"/><polyline points="10 9 9 9 8 9"/></svg>892 Words</p><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>2023-08-07 11:00</p></div><hr><div class=sharing-buttons><a class=resp-sharing-button__link href="https://twitter.com/intent/tweet/?url=https%3a%2f%2ftimotk.github.io%2fposts%2f2023%2f08%2fenforce-and-validate-llm-output-with-pydantic%2f" target=_blank rel=noopener aria-label title="Share on twitter"><div class="resp-sharing-button resp-sharing-button--twitter resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg></div></div></a><a class=resp-sharing-button__link href="mailto:?subject=Enforce%20and%20Validate%20LLM%20Output%20with%20Pydantic&amp;body=https%3a%2f%2ftimotk.github.io%2fposts%2f2023%2f08%2fenforce-and-validate-llm-output-with-pydantic%2f" target=_self rel=noopener aria-label title="Share via email"><div class="resp-sharing-button resp-sharing-button--email resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></div></div></a><a class=resp-sharing-button__link href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftimotk.github.io%2fposts%2f2023%2f08%2fenforce-and-validate-llm-output-with-pydantic%2f&amp;title=Enforce%20and%20Validate%20LLM%20Output%20with%20Pydantic&amp;summary=Enforce%20and%20Validate%20LLM%20Output%20with%20Pydantic&amp;source=https%3a%2f%2ftimotk.github.io%2fposts%2f2023%2f08%2fenforce-and-validate-llm-output-with-pydantic%2f" target=_blank rel=noopener aria-label title="Share on linkedin"><div class="resp-sharing-button resp-sharing-button--linkedin resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></div></div></a><a class=resp-sharing-button__link href="https://reddit.com/submit/?url=https%3a%2f%2ftimotk.github.io%2fposts%2f2023%2f08%2fenforce-and-validate-llm-output-with-pydantic%2f&amp;resubmit=true&amp;title=Enforce%20and%20Validate%20LLM%20Output%20with%20Pydantic" target=_blank rel=noopener aria-label title="Share on reddit"><div class="resp-sharing-button resp-sharing-button--reddit resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentcolor" stroke="none"><path d="M12 0A12 12 0 000 12a12 12 0 0012 12 12 12 0 0012-12A12 12 0 0012 0zm5.01 4.744c.688.0 1.25.561 1.25 1.249a1.25 1.25.0 01-2.498.056l-2.597-.547-.8 3.747c1.824.07 3.48.632 4.674 1.488.308-.309.73-.491 1.207-.491.968.0 1.754.786 1.754 1.754.0.716-.435 1.333-1.01 1.614a3.111 3.111.0 01.042.52c0 2.694-3.13 4.87-7.004 4.87s-7.004-2.176-7.004-4.87c0-.183.015-.366.043-.534A1.748 1.748.0 014.028 12c0-.968.786-1.754 1.754-1.754.463.0.898.196 1.207.49 1.207-.883 2.878-1.43 4.744-1.487l.885-4.182a.342.342.0 01.14-.197.35.35.0 01.238-.042l2.906.617a1.214 1.214.0 011.108-.701zM9.25 12C8.561 12 8 12.562 8 13.25c0 .687.561 1.248 1.25 1.248.687.0 1.248-.561 1.248-1.249S9.937 12 9.249 12zm5.5.0c-.687.0-1.248.561-1.248 1.25.0.687.561 1.248 1.249 1.248S16 13.937 16 13.249c0-.687-.562-1.249-1.25-1.249zm-5.466 3.99a.327.327.0 00-.231.094.33.33.0 000 .463c.842.842 2.484.913 2.961.913s2.105-.056 2.961-.913a.361.361.0 00.029-.463.33.33.0 00-.464.0c-.547.533-1.684.73-2.512.73-.828.0-1.979-.196-2.512-.73a.326.326.0 00-.232-.095z"/></svg></div></div></a><a class=resp-sharing-button__link href="whatsapp://send?text=Enforce%20and%20Validate%20LLM%20Output%20with%20Pydantic%20https%3a%2f%2ftimotk.github.io%2fposts%2f2023%2f08%2fenforce-and-validate-llm-output-with-pydantic%2f" target=_blank rel=noopener aria-label title="Share on whatsapp"><div class="resp-sharing-button resp-sharing-button--whatsapp resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentcolor" stroke="none" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"><path d="M17.472 14.382c-.297-.149-1.758-.867-2.03-.967-.273-.099-.471-.148-.67.15-.197.297-.767.966-.94 1.164-.173.199-.347.223-.644.075-.297-.15-1.255-.463-2.39-1.475-.883-.788-1.48-1.761-1.653-2.059-.173-.297-.018-.458.13-.606.134-.133.298-.347.446-.52.149-.174.198-.298.298-.497.099-.198.05-.371-.025-.52-.075-.149-.669-1.612-.916-2.207-.242-.579-.487-.5-.669-.51-.173-.008-.371-.01-.57-.01-.198.0-.52.074-.792.372-.272.297-1.04 1.016-1.04 2.479.0 1.462 1.065 2.875 1.213 3.074.149.198 2.096 3.2 5.077 4.487.709.306 1.262.489 1.694.625.712.227 1.36.195 1.871.118.571-.085 1.758-.719 2.006-1.413.248-.694.248-1.289.173-1.413-.074-.124-.272-.198-.57-.347m-5.421 7.403h-.004a9.87 9.87.0 01-5.031-1.378l-.361-.214-3.741.982.998-3.648-.235-.374a9.86 9.86.0 01-1.51-5.26c.001-5.45 4.436-9.884 9.888-9.884 2.64.0 5.122 1.03 6.988 2.898a9.825 9.825.0 012.893 6.994c-.003 5.45-4.437 9.884-9.885 9.884m8.413-18.297A11.815 11.815.0 0012.05.0C5.495.0.16 5.335.157 11.892c0 2.096.547 4.142 1.588 5.945L.057 24l6.305-1.654a11.882 11.882.0 005.683 1.448h.005c6.554.0 11.89-5.335 11.893-11.893a11.821 11.821.0 00-3.48-8.413z"/></svg></div></div></a><a class=resp-sharing-button__link href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2ftimotk.github.io%2fposts%2f2023%2f08%2fenforce-and-validate-llm-output-with-pydantic%2f&amp;t=Enforce%20and%20Validate%20LLM%20Output%20with%20Pydantic" target=_blank rel=noopener aria-label title="Share on hacker news"><div class="resp-sharing-button resp-sharing-button--hackernews resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentcolor" stroke="none"><path d="M0 24V0h24v24H0zM6.951 5.896l4.112 7.708v5.064h1.583v-4.972l4.148-7.799h-1.749l-2.457 4.875c-.372.745-.688 1.434-.688 1.434s-.297-.708-.651-1.434L8.831 5.896h-1.88z"/></svg></div></div></a><a class=resp-sharing-button__link href="https://telegram.me/share/url?text=Enforce%20and%20Validate%20LLM%20Output%20with%20Pydantic&amp;url=https%3a%2f%2ftimotk.github.io%2fposts%2f2023%2f08%2fenforce-and-validate-llm-output-with-pydantic%2f" target=_blank rel=noopener aria-label title="Share on telegram"><div class="resp-sharing-button resp-sharing-button--telegram resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="22" y1="2" x2="11" y2="13"/><polygon points="22 2 15 22 11 13 2 9 22 2"/></svg></div></div></a></div><div class=pagination><div class=pagination__buttons><span class="button next"><a href=https://timotk.github.io/posts/2022/06/a-terminal-spinner-in-python/><span class=button__text>A Terminal Spinner in Python</span>
<span class=button__icon>â†’</span></a></span></div></div></main></div><footer class=footer><div class=footer__inner><div class=footer__content><span>&copy; 2023</span>
<span><a href=https://timotk.github.io>Timo</a></span>
<span><a href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank rel=noopener>CC BY-NC 4.0</a></span>
<span><a href=https://timotk.github.io/posts/index.xml target=_blank title=rss><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></span></div></div><div class=footer__inner><div class=footer__content><span>Powered by <a href=http://gohugo.io>Hugo</a></span><span>Made with &#10084; by <a href=https://github.com/timotk>Timo</a></span></div></div></footer></div><script type=text/javascript src=https://timotk.github.io/bundle.min.bb2c6bc3ed452ca4759660e4020811f248bc2320081559e8a32d8b0092773852941133639d35e8370d03d3ddaa750b1edd6b343c5bd22a55d5bdeae8f648f49b.js integrity="sha512-uyxrw+1FLKR1lmDkAggR8ki8IyAIFVnooy2LAJJ3OFKUETNjnTXoNw0D092qdQse3Ws0PFvSKlXVvero9kj0mw=="></script></body></html>