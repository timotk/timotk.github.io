[{"content":"Large Language Models (LLMs) excel in generating text but often struggle to produce structured output. By leveraging Pydantic\u0026rsquo;s type validation and prompt engineering, we can enforce and validate the output generated by LLMs.\nAll code examples in this blog post are written in Python. The LLM used is OpenAI\u0026rsquo;s gpt-3.5-turbo.\nQuery the LLM To query the LLM, we use the following function:\nimport openai def query(prompt: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Query the LLM with the given prompt.\u0026#34;\u0026#34;\u0026#34; completion = openai.ChatCompletion.create( model=\u0026#34;gpt-3.5-turbo\u0026#34;, messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: prompt, } ], temperature=0.0, ) return completion.choices[0].message.content Query the model We can query the model with a simple question:\nresponse = query(\u0026#34;What is the largest planet in our solar system?\u0026#34;) print(response) \u0026#39;The largest planet in our solar system is Jupiter.\u0026#39; Enforcing JSON output with a prompt In our prompt, we can ask the LLM to respond in a certain format:\nprompt = \u0026#34;\u0026#34;\u0026#34; I will ask you questions and you will respond. Your response should be in the following format: ```json { \u0026#34;thought\u0026#34;: \u0026#34;How you think about the question\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;The answer to the question\u0026#34; } ``` \u0026#34;\u0026#34;\u0026#34; Then, we query the model:\nquestion = \u0026#34;What is the largest planet in our solar system?\u0026#34; response = query(prompt + question) print(response) \u0026#39;{ \u0026#34;thought\u0026#34;: \u0026#34;This is a factual question that can be answered with scientific knowledge.\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;The largest planet in our solar system is Jupiter.\u0026#34; }\u0026#39; This is great, because we can easily parse the structured output:\nimport json parsed_response = json.loads(response) print(parsed_response[\u0026#34;answer\u0026#34;]) \u0026#39;The largest planet in our solar system is Jupiter.\u0026#39; Validating the output from pydantic import BaseModel class ThoughtAnswerResponse(BaseModel): thought: str answer: str raw_response = query(prompt) # Note: When you are using pydantic\u0026lt;2.0, use parse_raw instead of model_validate_json validated_response = ThoughtAnswerResponse.model_validate_json(raw_response) print(validated_response) thought=\u0026#39;This is a factual question that can be answered with scientific knowledge.\u0026#39; answer=\u0026#39;The largest planet in our solar system is Jupiter.\u0026#39; print(type(validated_response)) \u0026lt;class \u0026#39;ThoughtAnswerResponse\u0026#39;\u0026gt; Using the Pydantic model in the prompt At this moment, we describe our response format in two places:\na JSON description in our prompt a corresponding Pydantic model When we want to update the response format, we need to change both the prompt and the Pydantic model. This can cause inconsistencies.\nWe can solve this by exporting the Pydantic model to a JSON schema and adding the schema to the prompt. This will make the response and the Pydantic model consistent.\nresponse_schema_dict = ThoughtAnswerResponse.model_json_schema() response_schema_json = json.dumps(response_schema_dict, indent=2) prompt = f\u0026#34;\u0026#34;\u0026#34; I will ask you questions, and you will respond. Your response should be in the following format: ```json {response_schema_json} ``` \u0026#34;\u0026#34;\u0026#34; The prompt will now look like this:\nI will ask you questions, and you will respond. Your response should be in the following format: ```json { \u0026#34;properties\u0026#34;: { \u0026#34;thought\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Thought\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;answer\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;Answer\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;required\u0026#34;: [\u0026#34;thought\u0026#34;, \u0026#34;answer\u0026#34;], \u0026#34;title\u0026#34;: \u0026#34;ThoughtAnswerResponse\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34; } The response will look like this:\n{ \u0026#34;thought\u0026#34;: \u0026#34;The largest planet in our solar system is Jupiter.\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;Jupiter\u0026#34; } Now, whenever you change the Pydantic model, the corresponding schema will be put in the prompt. Note that the schema has become more complex than it was before. One benefit is that it allows us to be more specific in what responses we require.\nError handling The LLM may still produce results that are not consistent with our model. We can add some code to catch this:\nfrom pydantic import ValidationError try: validated_response = ThoughtAnswerResponse.model_validate_json(raw_response) except ValidationError as e: print(\u0026#34;Unable to validate LLM response.\u0026#34;) # Add your own error handling here raise e Enforce specific values using a Literal Sometimes, you want to enforce the use of specific values for a given field. We add the field \u0026ldquo;difficulty\u0026rdquo; to our response object. The LLM should use it to provide information about the difficulty of the question. In a regular prompt, we would do the following:\nprompt = \u0026#34;\u0026#34;\u0026#34;Your response should be in the following format: ```json { \u0026#34;thought\u0026#34;: \u0026#34;How you think about the question\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;The answer to the question\u0026#34;, \u0026#34;difficulty\u0026#34;: \u0026#34;How difficult the question was. One of easy, medium or hard\u0026#34; } ``` \u0026#34;\u0026#34;\u0026#34; Of course, the model could potentially still use other values. To validate it, we would need to write custom code.\nWith Pydantic, it is a lot easier. We create a new type called Difficulty using a Literal. A Literal allows us to specify the use of a select list of values. We add a Difficulty type hint to the difficulty field in our Pydantic model:\nfrom typing import Literal from pydantic import BaseModel # We create a new type Difficulty = Literal[\u0026#34;easy\u0026#34;, \u0026#34;medium\u0026#34;, \u0026#34;hard\u0026#34;] class ThoughtAnswerResponse(BaseModel): thought: str answer: str difficulty: Difficulty The LLM responds may respond with a value we do not allow:\n{ \u0026#34;thought\u0026#34;: \u0026#34;The largest planet in our solar system is Jupiter.\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;Jupiter\u0026#34;, \u0026#34;difficulty\u0026#34;: \u0026#34;Unknown\u0026#34; } When we parse this result, Pydantic will validate the values for the difficulty field. Unknown does not match one of the values specified in the Literal type we have defined. So we get the following error:\nvalidated_response = ThoughtAnswerResponse.model_validate_json(response) ValidationError: 1 validation error for ThoughtAnswerResponse difficulty Input should be \u0026#39;easy\u0026#39;, \u0026#39;medium\u0026#39; or \u0026#39;hard\u0026#39; [type=literal_error, input_value=\u0026#39;Unknown\u0026#39;, input_type=str] Conclusion By using Pydantic and prompt engineering, you can enforce and validate the output of LLMs. This provides you with greater control of the LLM output and allow you to build more robust AI systems.\n","permalink":"https://timotk.github.io/posts/enforce-validate-llm-output-pydantic/","summary":"Large Language Models (LLMs) excel in generating text but often struggle to produce structured output. By leveraging Pydantic\u0026rsquo;s type validation and prompt engineering, we can enforce and validate the output generated by LLMs.\nAll code examples in this blog post are written in Python. The LLM used is OpenAI\u0026rsquo;s gpt-3.5-turbo.\nQuery the LLM To query the LLM, we use the following function:\nimport openai def query(prompt: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Query the LLM with the given prompt.","title":"Enforce and Validate LLM Output with Pydantic"},{"content":"We are going to build a spinning status indicator that runs while other code is executing.\nIt will look like this: Why? You\u0026rsquo;ve got some code that takes a while to run.\nimport time import random def slow_func(): seconds = random.randint(2, 5) time.sleep(seconds) print(\u0026#34;Done!\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: slow_func() Now, when you execute this, you\u0026rsquo;ll see the following: You\u0026rsquo;ll wonder whether your code or your system is working correctly or frozen. Who knows?\nIntroducing a spinner Ideally, we\u0026rsquo;d like to show some sort of activity while our code is executing. We can do that with a spinner. To create a spinner, we can use:\nimport time import itertools def spin(): spinners = [\u0026#34;|\u0026#34;, \u0026#34;/\u0026#34;, \u0026#34;-\u0026#34;, \u0026#34;\\\\\u0026#34;] for c in itertools.cycle(spinners): print(f\u0026#34;\\r{c}\u0026#34;, end=\u0026#34;\u0026#34;) time.sleep(0.1) We introduce a list of spinners (| / - \\ ). The double backslash is used because of escaping. Using itertools.cycle, we can create an endless cycle of our spinner elements. In each iteration, we print one of characters. By default, Python ends a print statement with a newline. We disable that by printing an empty string (end=\u0026quot;\u0026quot;) By putting \\r in front of our character, we move our cursor back to the start of the line. This is called a carriage return. We sleep for 100ms. Combine the spinner with the code Now, combining them can be done like this:\nspin() slow_func() But obviously this does not work, since our code executes sequentially. First the spinner runs to completion, then slow_func will run. Due to the endless nature of itertools.cycle, our code in spin() never stops.\nTo solve this, we can run our spinner in its own thread, which allows us to run code in parallel:\nimport threading if __name__ == \u0026#39;__main__\u0026#39;: thread = threading.Thread(target=spin) thread.daemon = True thread.start() slow_func() We start a new thread, with the spin function as its target. We set thread.daemon to True, to make the thread run in the background. We start the thread. We call our slow function Here\u0026rsquo;s what it looks like: Making it awesome If you want to reuse your code, it wouldn\u0026rsquo;t be so nice. To fix that, we can introduce a context manager. This will make usage look like this:\nwith Spinner(): slow_func() Here\u0026rsquo;s how we write the context manager:\nclass Spinner: def __init__(self): self.running = False self.thread = threading.Thread(target=self.spin) self.thread.daemon = True def spin(self): spinners = [\u0026#34;|\u0026#34;, \u0026#34;/\u0026#34;, \u0026#34;-\u0026#34;, \u0026#34;\\\\\u0026#34;] for c in itertools.cycle(spinners): if not self.running: print(\u0026#34;\\r\u0026#34;, end=\u0026#34;\u0026#34;) break print(f\u0026#34;\\r{c}\u0026#34;, end=\u0026#34;\u0026#34;) time.sleep(0.1) def __enter__(self): self.running = True self.thread.start() def __exit__(self, exc_type, exc_val, exc_tb): self.running = False The logic is:\nWe initialize the instance with Spinner(). This calls __init__(), which sets running to False and creates the thread. After the with Spinner(): line, Spinner.__enter__() gets called. We now enter the context and the thread starts running. Our slow function runs. Meanwhile, every 100ms, a spin character gets printed. Our slow function ends and we exit the with block. Now, Spinner.__exit__() gets called. Running will be set to False, which means the spin() method will break out of its loop, once it detects self.running is False. Further improvements We can make our code even more dynamic, by allowing you to set the spin timeout and the spinners during class initialization. Here\u0026rsquo;s the full code:\nimport itertools import threading import time class Spinner: def __init__(self, timeout: float = 0.1, spinners: list = [\u0026#34;|\u0026#34;, \u0026#34;/\u0026#34;, \u0026#34;-\u0026#34;, \u0026#34;\\\\\u0026#34;]): self.timeout = timeout self.spinners = spinners self.running = False self.thread = threading.Thread(target=self.spin) self.thread.daemon = True def spin(self): for c in itertools.cycle(self.spinners): if not self.running: print(\u0026#34;\\r\u0026#34;, end=\u0026#34;\u0026#34;) break print(f\u0026#34;\\r{c}\u0026#34;, end=\u0026#34;\u0026#34;) time.sleep(self.timeout) def __enter__(self): self.running = True self.thread.start() def __exit__(self, exc_type, exc_val, exc_tb): self.running = False You could use any spinners you like, for example:\n[\u0026#34;â¢¿\u0026#34;, \u0026#34;â£»\u0026#34;, \u0026#34;â£½\u0026#34;, \u0026#34;â£¾\u0026#34;, \u0026#34;â£·\u0026#34;, \u0026#34;â£¯\u0026#34;, \u0026#34;â£Ÿ\u0026#34;, \u0026#34;â¡¿\u0026#34;] [\u0026#34;ðŸ‘†\u0026#34;, \u0026#34;ðŸ‘‰\u0026#34;, \u0026#34;ðŸ‘‡\u0026#34;, \u0026#34;ðŸ‘ˆ\u0026#34;] [\u0026#34;| \u0026#34;, \u0026#34; | \u0026#34;, \u0026#34; | \u0026#34;, \u0026#34; |\u0026#34;] You can find many more examples online, or you can simply make your own.\nFinal thoughts I hoped you learned something about how we can indicate activity while you are running your program interactively. If you need a more extensive approach, you can use a library like rich.\n","permalink":"https://timotk.github.io/posts/a-terminal-spinner-in-python/","summary":"We are going to build a spinning status indicator that runs while other code is executing.\nIt will look like this: Why? You\u0026rsquo;ve got some code that takes a while to run.\nimport time import random def slow_func(): seconds = random.randint(2, 5) time.sleep(seconds) print(\u0026#34;Done!\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: slow_func() Now, when you execute this, you\u0026rsquo;ll see the following: You\u0026rsquo;ll wonder whether your code or your system is working correctly or frozen.","title":"A Terminal Spinner in Python"},{"content":"Hi, I am Timo.\n","permalink":"https://timotk.github.io/about/","summary":"Hi, I am Timo.","title":"About"}]