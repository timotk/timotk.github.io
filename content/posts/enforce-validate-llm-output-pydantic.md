---
title: "Enforce Validate Llm Output Pydantic"
date: 2023-08-03T14:11:58+02:00
draft: true
toc: false
images:
tags:
  - untagged
---

 # Introduction
Large Language Models (LLMs) excel in generating text but often struggle with producing structured output. By leveraging Pydantic and prompt engineering, we can enforce and validate the output generated by LLMs.

*All code examples in this blogpost are written in Python. The LLM used is  [OpenAI's gpt-3.5-turbo](https://platform.openai.com/docs/guides/gpt).*

# Query the LLM
For querying the LLM, we use the following function:
```python
import openai

def query(prompt: str) -> str:
	"""Query the LLM with the given prompt."""
	completion = openai.ChatCompletion.create(
		model="gpt-3.5-turbo",
		messages=[
			{
				"role": "user",
				"content": prompt,
			}
		],
		temperature=0.0,
	)
	return completion.choices[0].message.content
```
# Enforcing JSON output with a prompt
In our prompt, we can ask the LLM to respond in a certain format:
````python
prompt = """
I will ask you questions and you will respond. Your response should be in the following format:
```json
{
    "thought": "How you think about the question",
    "answer": "The answer to the question",
}
```
"""
````

Then, we query the model:
```python
question = "What is the current year?"
response = query(prompt + question)
print(response)
'{
	"thought": "Checking the current year",  
	"answer": "2022"
}'
```

This is great, because we can easily parse the structured output:
```python
import json

parsed_response = json.loads(response)
print(parsed_response["answer"])
2022
```
# Validating the output

```python
from pydantic import BaseModel

class ThoughtAnswerResponse(BaseModel):
    thought: str
    answer: str

raw_response = query(prompt)

validated_response = ThoughtAnswerResponse.model_validate_json(raw_response)  # When you are using pydantic<2.0, use parse_raw instead of model_validate_json

print(validated_response)
thought='This is a straightforward question that requires a simple answer.' answer='2022'

print(type(validated_response))
<class 'ThoughtAnswerResponse'>
```
# Using the Pydantic model in the prompt
At this moment, we describe our response format in two places:
- a JSON description in our prompt
- a corresponding Pydantic model

When we want to change the response, we need to change both the class and the prompt. This can cause inconsistencies.

We can [export the Pydantic model to a JSON schema](https://docs.pydantic.dev/latest/usage/json_schema/). If we add this schema to the prompt, the response and the Pydantic model are consistent.

````python
response_schema_dict = ThoughtAnswerResponse.model_json_schema()
response_schema_json = json.dumps(response_schema_dict, indent=2)

prompt = f"""
I will ask you questions, and you will respond.
Your response should be in the following format:
```json
{response_schema_json}
"""
```
````

The prompt will now look like this:
```
I will ask you questions, and you will respond.Your response should be in the following format: 
```json
{
  "properties": {
    "thought": { "title": "Thought", "type": "string" },
    "answer": { "title": "Answer", "type": "string" }
  },
  "required": ["thought", "answer"],
  "title": "ThoughtAnswerResponse",
  "type": "object"
}
```

Also, whenever you change the Pydantic model, the corresponding schema will be put in the prompt.

*The schema in the prompt has become more complex. It allows us to be more specific in what responses we require.*

# Error handling
The LLM may still produce results that are not consistent with our model. Therefore, we can add some code to catch this:
```python
from pydantic import ValidationError

...

try:
   validated_response = ThoughtAnswerResponse.model_validate_json(raw_response)
except ValidationError as e:
    print("Unable to validate LLM response.")
    # TODO: Add your own error handling here
	raise e
```
# Bonus: Enforce specific values using a Literal
Sometimes, you want to enforce the use of specific values for a given field. We add the field "difficulty" to our response object. The LLM should use it to provide information about the difficulty of the question. In a regular prompt, we would do the following:
```json
Your response should be in the following format:
{
    "thought": "How you think about the question",
    "answer": "The answer to the question",
    "difficulty": "How difficult the question was. One of easy, medium or hard"
}
```

Of course, the model can still use other values. To validate it, we would need to write custom code.

With Pydantic, it's much easier. We create a new Difficulty type using a Literal. In the Literal, we provide the required values. Then, we add the Difficulty type hint to the difficulty field in our Pydantic model:
```python
from typing import Literal

from pydantic import BaseModel


Difficulty = Literal["easy", "medium", "hard"]

class ThoughtAnswerResponse(BaseModel):
    thought: str
    answer: str
    difficulty: Difficulty
```

The LLM may respond with the following response:
```json
{
	"thought": "Checking the current year",  
	"answer": "2022",
	"difficulty": "Unknown"
}
```

When we parse this result, Pydantic will validate the values for the `difficulty` field. Clearly, `Unknown` does not match one of the values specified in the Literal, so we get an error:
```python
pydantic.error_wrappers.ValidationError: 1 validation error for ThoughtAnswerResponse
difficulty
  unexpected value; permitted: 'easy', 'medium', 'hard' (type=value_error.const; given=Unknown; permitted=('easy', 'medium', 'hard'))
```

# Conclusion
<TODO>