<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>llm on Timo&#39;s Blog</title>
    <link>https://timotk.github.io/tags/llm/</link>
    <description>Recent content in llm on Timo&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 07 Aug 2023 13:00:00 +0200</lastBuildDate><atom:link href="https://timotk.github.io/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Enforce and Validate LLM Output with Pydantic</title>
      <link>https://timotk.github.io/posts/enforce-validate-llm-output-pydantic/</link>
      <pubDate>Mon, 07 Aug 2023 13:00:00 +0200</pubDate>
      
      <guid>https://timotk.github.io/posts/enforce-validate-llm-output-pydantic/</guid>
      <description>Large Language Models (LLMs) excel in generating text but often struggle to produce structured output. By leveraging Pydantic&amp;rsquo;s type validation and prompt engineering, we can enforce and validate the output generated by LLMs.
All code examples in this blog post are written in Python. The LLM used is OpenAI&amp;rsquo;s gpt-3.5-turbo.
Query the LLM To query the LLM, we use the following function:
import openai def query(prompt: str) -&amp;gt; str: &amp;#34;&amp;#34;&amp;#34;Query the LLM with the given prompt.</description>
    </item>
    
  </channel>
</rss>
